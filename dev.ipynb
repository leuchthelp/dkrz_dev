{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T10:06:19.818632Z",
     "start_time": "2024-11-07T10:06:19.816309Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "leucht: RTC timebase, using 213 picos/cycle from /proc instead of the detected 195 picos/cycle\n",
      "\n",
      "leucht.26733RTC timebase, using 213 picos/cycle from /proc instead of the detected 195 picos/cycle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from netCDF4 import Dataset\n",
    "import zarr, h5py, fsspec\n",
    "import xarray as xr\n",
    "import os.path\n",
    "from mpi4py import MPI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d441978f431651b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T10:06:24.270989Z",
     "start_time": "2024-11-07T10:06:24.266269Z"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "if __name__ == '__main__':\n",
    "    #Todo read up on hdf5, zarr, netcdf, xarray, mpi, lustre, ceph\n",
    "    # and check back with dkrz their structure, code conventions, datasets\n",
    "\n",
    "    # Todo setup test cases proactively to ensure proper comparability and best practice\n",
    "\n",
    "    #Todo use netcdf dataset for testing (for now) should already bet hdf5\n",
    "    dest_hdf = Dataset(\"data/source.nc\", \"w\", format=\"NETCDF4\")\n",
    "    print(dest_hdf.data_model)\n",
    "    dest_hdf.close()\n",
    "\n",
    "    #Todo import netcdf data through hdf5 to zarr (for now)\n",
    "    dest_zarr = zarr.open_group('data/example2.zarr', mode='w')\n",
    "    zarr.copy_all(dest_hdf, dest_zarr)\n",
    "    dest_zarr.tree()\n",
    "\n",
    "    #Todo conversion to netcdf\n",
    "\n",
    "    #for hdf5 nothing needs to be done\n",
    "\n",
    "    #for zarr (wip). netcdf is developing its own implementation\n",
    "    # but that isn't available yet so I will work on something myself in the meantime\n",
    "\n",
    "    #Todo setup Lustre and Ceph (need info on that)\n",
    "\n",
    "    #Todo setup benchmark (prob compression, filesize, access-time, r/w-time) for sequential access\n",
    "\n",
    "    #Todo setup benchmark for parallel access\n",
    "\n",
    "    #Todo setup benchmark for random access\n",
    "\n",
    "    #Todo setup benchmark for parallel with subfileing and async / I/O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316211b7bd13977d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T10:06:33.237088Z",
     "start_time": "2024-11-07T10:06:27.742801Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "(x_img_train, y_label_train), (x_img_test, y_label_test) = cifar10.load_data()\n",
    "\n",
    "# create HDF5 file\n",
    "with h5py.File('data/test_ds.hdf5', 'w') as hf:\n",
    "    u = hf.create_dataset(\"u\", data=np.random.rand(1_000_000, 10, 10, 1), shape=(1_000_000, 10, 10, 1), compression=\"gzip\", chunks=True)\n",
    "    v = hf.create_dataset(\"v\", data=np.random.rand(1_000_000, 10, 10, 1), shape=(1_000_000, 10, 10, 1), compression=\"gzip\", chunks=True)\n",
    "    w = hf.create_dataset(\"w\", data=np.random.rand(1_000_000, 10, 10, 1), shape=(1_000_000, 10, 10, 1), compression=\"gzip\", chunks=True)\n",
    "    x = hf.create_dataset(\"x\", data=np.random.rand(1_000_000, 10, 10, 1), shape=(1_000_000, 10, 10, 1), compression=\"gzip\", chunks=True)\n",
    "    y = hf.create_dataset(\"y\", data=np.random.rand(1_000_000, 10, 10, 1), shape=(1_000_000, 10, 10, 1), compression=\"gzip\", chunks=True)\n",
    "\n",
    "hf.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93924c7b041d484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T10:06:46.023619Z",
     "start_time": "2024-11-07T10:06:45.677187Z"
    }
   },
   "outputs": [],
   "source": [
    "#Todo prepare dataset for testing\n",
    "\n",
    "# filepath_zarr = 'https://power-datastore.s3.amazonaws.com/v9/climatology/power_901_climatology_meteorology_utc.zarr'\n",
    "# \n",
    "# filepath_mapped_zarr = fsspec.get_mapper(filepath_zarr)\n",
    "\n",
    "ds = xr.open_dataset(\"data/test_ds.hdf5\", engine=\"h5netcdf\", phony_dims=\"access\")\n",
    "\n",
    "ds.to_netcdf('data/test_dataset.nc', mode=\"w\")\n",
    "\n",
    "ds.to_zarr(\"data/test_dataset.zarr\", mode=\"w\")\n",
    "\n",
    "filepath_mapped_zarr = 'data/test_dataset.zarr'\n",
    "\n",
    "filepath_nc = 'data/test_dataset.nc'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ff585ad6e144f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.88 ms, sys: 0 ns, total: 1.88 ms\n",
      "Wall time: 1.38 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "ds_zarr = xr.open_zarr(filepath_mapped_zarr, consolidated=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d37e8dda5b5ccdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 ms, sys: 0 ns, total: 19 ms\n",
      "Wall time: 16.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "max_var = ds_zarr[\"x\"].max().compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a1222041420b736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.08 ms, sys: 0 ns, total: 9.08 ms\n",
      "Wall time: 7.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds_hdf5 = xr.open_dataset(filename_or_obj=filepath_nc, engine=\"h5netcdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcd381f7ab539d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.05 ms, sys: 10.7 ms, total: 17.7 ms\n",
      "Wall time: 15.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_var = ds_hdf5[\"x\"].max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b605cf91-8b72-4cd3-a34f-19c3a085cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "import xarray as xr\n",
    "ds1 = xr.open_dataset(filename_or_obj=\"data/4f6e0dccfcfe4f76f1e64f187cc0551c.nc\", engine=\"netcdf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c16c0-c4e1-499c-b6c0-98cd3a9c2787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
